{"cells":[{"cell_type":"code","execution_count":48,"id":"9dd358e9","metadata":{"scrolled":true},"outputs":[{"name":"stderr","output_type":"stream","text":["                                                                                \r"]},{"name":"stdout","output_type":"stream","text":["+---------------+------------+------------+------------+\n","|passenger_count|pulocationid|dolocationid|total_amount|\n","+---------------+------------+------------+------------+\n","|            1.0|       151.0|       239.0|        9.95|\n","|            1.0|       239.0|       246.0|        16.3|\n","|            3.0|       236.0|       236.0|         5.8|\n","|            5.0|       193.0|       193.0|        7.55|\n","|            5.0|       193.0|       193.0|       55.55|\n","|            5.0|       193.0|       193.0|       13.31|\n","|            5.0|       193.0|       193.0|       55.55|\n","|            1.0|       163.0|       229.0|        9.05|\n","|            1.0|       229.0|         7.0|        18.5|\n","|            2.0|       141.0|       234.0|        13.0|\n","+---------------+------------+------------+------------+\n","only showing top 10 rows\n","\n"]}],"source":["# 1. Create a dataset that only contains passenger_count (4th col), pulocationid (8th col), dolocationid (9th col), and total_amount (17th col) based on the 2019-01-h1.csv dataset. In the Jupyter notebook, show the first 10 entries in the created dataset.\n","\n","filePath = \"gs://dataproc-staging-us-central1-176366540446-vnmuytv8/data/a4data.csv\"\n","a4DF = spark.read.csv(filePath, header=True, inferSchema=True)\n","a4DF.show(10)"]},{"cell_type":"code","execution_count":49,"id":"e5b4697d","metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["[Stage 189:============================>                            (1 + 1) / 2]\r"]},{"name":"stdout","output_type":"stream","text":["There are 2920360 rows in the training set, and 730639 in the test set\n"]},{"name":"stderr","output_type":"stream","text":["\r","                                                                                \r"]}],"source":["# 2. Create trainDF and testDF.\n","trainDF, testDF = a4DF.randomSplit([.8, .2], seed=42)\n","print(f\"There are {trainDF.count()} rows in the training set, and {testDF.count()} in the test set\")"]},{"cell_type":"code","execution_count":50,"id":"64446ce1","metadata":{},"outputs":[],"source":["# 3. Create a decision tree regressor to predict total_amount from the other three features.\n","from pyspark.ml.regression import DecisionTreeRegressor\n","\n","dtr = DecisionTreeRegressor(featuresCol=\"features\", labelCol=\"total_amount\").setMaxBins(300)"]},{"cell_type":"code","execution_count":51,"id":"b59b83c0","metadata":{},"outputs":[],"source":["# 4. Create a pipeline.\n","from pyspark.ml import Pipeline\n","\n","feature_cols = [\"passenger_count\",\"pulocationid\",\"dolocationid\"]\n","vecAssembler = VectorAssembler(inputCols=feature_cols, outputCol=\"features\")\n","\n","pipeline = Pipeline(stages=[vecAssembler, dtr])"]},{"cell_type":"code","execution_count":52,"id":"60247dde","metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["                                                                                \r"]}],"source":["# 5. Train the model\n","pipelineModel = pipeline.fit(trainDF)"]},{"cell_type":"code","execution_count":53,"id":"6d82c30a","metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["[Stage 206:============================>                            (1 + 1) / 2]\r"]},{"name":"stdout","output_type":"stream","text":["+-----------------+------------+-----------------+\n","|         features|total_amount|       prediction|\n","+-----------------+------------+-----------------+\n","|[1.0,116.0,151.0]|         9.8|13.58510172986951|\n","| [1.0,43.0,238.0]|         9.8|13.58510172986951|\n","|[1.0,140.0,233.0]|         9.3|12.02480847692699|\n","|  [2.0,87.0,45.0]|         7.3|13.58510172986951|\n","| [1.0,43.0,230.0]|       16.56|13.58510172986951|\n","|  [2.0,50.0,48.0]|         8.5|13.58510172986951|\n","|  [1.0,79.0,90.0]|        11.0|13.58510172986951|\n","|[3.0,237.0,162.0]|         7.8|12.02480847692699|\n","|[6.0,143.0,163.0]|        7.56|12.02480847692699|\n","|[4.0,141.0,141.0]|         5.3|12.02480847692699|\n","+-----------------+------------+-----------------+\n","only showing top 10 rows\n","\n"]},{"name":"stderr","output_type":"stream","text":["\r","                                                                                \r"]}],"source":["# 6. Show the predicted results along with the three features in the notebook.\n","from pyspark.sql.functions import rand \n","# Rand() to show predictions without being sorted by features\n","predDF = pipelineModel.transform(testDF)\n","predDF.orderBy(rand()).select(\"features\", \"total_amount\", \"prediction\").show(10)"]},{"cell_type":"code","execution_count":54,"id":"917cbb73","metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["[Stage 207:============================>                            (1 + 1) / 2]\r"]},{"name":"stdout","output_type":"stream","text":["RMSE: 60.925502421048385\n"]},{"name":"stderr","output_type":"stream","text":["\r","                                                                                \r"]}],"source":["# 7. Evaluate the model with RMSE.\n","from pyspark.ml.evaluation import RegressionEvaluator\n","\n","regressionEvaluator = RegressionEvaluator(\n","    predictionCol=\"prediction\",\n","    labelCol=\"total_amount\",\n","    metricName=\"rmse\"\n",")\n","rmse = regressionEvaluator.evaluate(predDF)\n","print(f\"RMSE: {rmse}\")"]},{"cell_type":"code","execution_count":null,"id":"ada0ec2d","metadata":{},"outputs":[],"source":[]}],"metadata":{"kernelspec":{"display_name":"PySpark","language":"python","name":"pyspark"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.8"}},"nbformat":4,"nbformat_minor":5}